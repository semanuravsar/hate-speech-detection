import argparse
import itertools
import time
import pandas as pd
import torch
from torch.utils.data import DataLoader
from sklearn.metrics import accuracy_score, f1_score

from models.multitask_bert import MultiTaskBERT
from scripts.dataset_loaders import LatentHatredDataset, StereoSetDataset, ISarcasmDataset
from scripts.utils import load_checkpoint


@torch.no_grad()
def evaluate(model, dataloaders, device):
    model.eval()
    metrics = {}
    for task, loader in dataloaders.items():
        all_preds, all_labels = [], []
        for batch in loader:
            input_ids = batch["input_ids"].to(device)
            attention_mask = batch["attention_mask"].to(device)
            labels = batch["labels"].to(device)

            logits = model(input_ids, attention_mask, task=task)
            preds = torch.argmax(logits, dim=1)

            all_preds.extend(preds.cpu().tolist())
            all_labels.extend(labels.cpu().tolist())

        acc = accuracy_score(all_labels, all_preds)
        f1 = f1_score(all_labels, all_preds, average="macro")
        metrics[task] = {"accuracy": acc, "f1": f1}
    return metrics


def run_experiments():
    from scripts.train import main  # avoid circular import

    learning_rates   = [1e-5, 2e-5]
    dropouts         = [0.1, 0.3]
    batch_sizes      = [8]
    epochs_list      = [3, 5]
    main_weights     = [1.0]
    stereo_weights   = [0.2, 0.5]
    sarcasm_weights  = [0.2, 0.5]

    best_score = 0
    best_config = None
    results = []

    for lr, dropout, batch_size, epochs, main_w, stereo_w, sarcasm_w in itertools.product(
        learning_rates, dropouts, batch_sizes, epochs_list, main_weights, stereo_weights, sarcasm_weights
    ):
        print(f"\nüöÄ Running: lr={lr}, dropout={dropout}, bs={batch_size}, ep={epochs}, mw={main_w}, sw={stereo_w}, sarw={sarcasm_w}")
        start_time = time.time()

        args = argparse.Namespace(
            dataset_dir="datasets",
            checkpoint_path=f"checkpoint_lr{lr}_do{dropout}_bs{batch_size}_ep{epochs}_mw{main_w}_sw{stereo_w}_sarw{sarcasm_w}.pt",
            resume=False,
            batch_size=batch_size,
            epochs=epochs,
            main_weight=main_w,
            stereo_weight=stereo_w,
            sarcasm_weight=sarcasm_w,
            lr=lr,
            dropout=dropout
        )

        try:
            main(args)

            device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
            model = MultiTaskBERT(dropout=dropout).to(device)
            load_checkpoint(model, path=args.checkpoint_path)

            hate_val = LatentHatredDataset(f"{args.dataset_dir}/latent_hatred_3class_sample_train.csv", split="val")
            stereo_val = StereoSetDataset(f"{args.dataset_dir}/stereoset_sample_train.csv", split="val")
            sarcasm_val = ISarcasmDataset(f"{args.dataset_dir}/isarcasm_sample_train.csv", split="val")

            dataloaders_val = {
                "main": DataLoader(hate_val, batch_size=args.batch_size),
                "stereo": DataLoader(stereo_val, batch_size=args.batch_size),
                "sarcasm": DataLoader(sarcasm_val, batch_size=args.batch_size)
            }

            metrics = evaluate(model, dataloaders_val, device=device)
            avg_f1 = sum([m["f1"] for m in metrics.values()]) / len(metrics)

            print("üìä Validation F1 Scores:")
            for task, m in metrics.items():
                print(f"{task}: F1 = {m['f1']:.4f}, Acc = {m['accuracy']:.4f}")

            results.append({
                "lr": lr,
                "dropout": dropout,
                "batch_size": batch_size,
                "epochs": epochs,
                "main_weight": main_w,
                "stereo_weight": stereo_w,
                "sarcasm_weight": sarcasm_w,
                "main_f1": metrics["main"]["f1"],
                "stereo_f1": metrics["stereo"]["f1"],
                "sarcasm_f1": metrics["sarcasm"]["f1"],
                "avg_f1": avg_f1
            })

            if avg_f1 > best_score:
                best_score = avg_f1
                best_config = {
                    "lr": lr, "dropout": dropout, "batch_size": batch_size,
                    "epochs": epochs, "main_weight": main_w,
                    "stereo_weight": stereo_w, "sarcasm_weight": sarcasm_w
                }

            print(f"‚úÖ Finished in {time.time() - start_time:.1f}s")

        except Exception as e:
            print(f"‚ùå Skipped config due to error: {e}")
            continue

    df = pd.DataFrame(results)
    df.to_csv("search_results.csv", index=False)
    print(f"\n‚úÖ Best avg F1: {best_score:.4f} with config:")
    print(best_config)
    print("üîç Full results saved to search_results.csv")


if __name__ == "__main__":
    run_experiments()